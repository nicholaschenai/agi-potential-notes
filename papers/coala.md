---
date: 2024-03-05
time: 18:46
author: 
title: Cognitive Architectures for Language Agents
created-date: 2024-03-05
tags: 
paper: https://arxiv.org/abs/2309.02427
code:
---
[[Repo (lit review)](https://github.com/ysymyth/awesome-language-agents)]
# Description of result
Concept paper which proposes "Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework"

Use the framework to "highlight gaps and propose actionable directions toward more capable language agents in the future."


## **Actionable insights**
- Working memory and reasoning: thinking beyond LLM prompt engineering: "the community should think about a structured working memory and systematic 'reasoning' actions that update working memory variables"
- Long-term memory: thinking beyond retrieval augmentation: "memory-augmented language agents can both read and write self-generated content autonomously"-- combine existing human knowledge with self-discovered and self maintained exp, knowledge and skills. example: coding agent starts off with human knowledge (semantic), problem solutions and test records (episodic), reflection n summaries on top of these exp (semantic), gradually expanding code lib that sttores useful methods eg Quicksort (procedural). A bit like bootstrapping AlphaGo off existing games then enhancing from self play?
- Learning: thinking beyond in-context learning or finetuning. (read this entire paragraph!)
	- learning better retrieval procedures could enable agents to better connect memorized information to new scenarios
	- recent expansion based techniques may be readily applied ('in what situations would this knowledge be useful?', and append the reasoning results to the knowledge to help later connect the knowledge to new situations)
	- Learning new learning and decision procedures may be necessary to empower agents to go beyond the limitations of human-provided code
	- explore smaller models for specific reasoning needs
	- unlearning!
	- combine multiple forms of learning
- Action space: thinking beyond external tools or actions.
	- impt to balance tradeoff for size of action space: larger means more expressivity/ capability, but harder decision making
	- Safety! currently limited to task-specific heuristics, but as agents are increasingly integrated into systems, "it will be necessary to clearly specify and ablate the agent's action space for worst-case scenario prediction and prevention"
- Decision making: thinking beyond action generation.
	- most works are still confined to proposing (or directly generating) a single action. It would be powerful to extend deliberate propose-evaluate-select schemes to "more complicated tasks with grounding and LT memory; enable learning of decision making procedures; and reduce the cost of extensive reasoning to facilitate more deliberate planning (potentially via smaller task-specific models for proposal or evaluation)"
	- "increased usage of reasoning toward complex decision making"
	- "solving known issues such as over-confidence and miscalibration, misalignment with human values or bias, hallucinations in self-evaluation, and lack of human-in-the-loop mechanisms in face of uncertainties"

---
# How it compares to previous work
Key contributions over previous CA
- Inclusion of LLMs leads to the addition of 'reasoning' actions, which can flexibly produce new knowledge and heuristics, replacing handcrafted rules in previous CAs
- makes text the de factor internal representation
- suggests vision-language models can simplify grounding by translating perceptual data to text

---
# Main strategies used to obtain results

## Memory
### Working memory

maintains active and readily available information as symbolic variables for the current decision cycle (scratch memory). eg perceptual inputs from grounding, knowledge generated by reasoning or retrieving from long term memory, other info from previous decision cycle.

### episodic memory
exp from earlier decision cycles, eg training data, history event flows, past game trajectories. Can be retrieved into working memory to support reasoning. can write new exp frm working to episoding memory as a form of learning
### semantic memory
agent's knowledge about the world and itself. (eg initialized from external db, RAG). past expamples tend to employ read-only semantic memory but  lang agents may write new knowledge from LLM reasoning into semantic memory as a form of learning.
### procedural memory 
- implicit knowledge in LLM weights, explicit knowledge in agent's code.
- explicit knowledge
	- procedures that implement actions
		- LLM accessed via reasoning action
		- code-based procedures retrieved
	- procedures that implement decision making
- must be initialized properly by designer, compared to episodic or semantic memory which can be initially empty
- possible to write to procedural memory but riskier than writing to semantic or episodic memory as this can introduce bugs or safety concerns (akin to Voyager writing wrong code in skills library)
## Actions
external actions: interact with external envs thru grounding
### internal actions: interact with internal memories
- reasoning: update short term working memory w LLM. read from AND write to working memory. to summarize n distill insights. support learning (by writing results into LT memory) or decision making (results as additional context for subsequent LLM calls)
- retrieval: read from long term memory into working memory. various implementations eg rule based, sparse, dense retrieval
	- eg Voyager uses dense retreival for retrieving skills frm lib, gen agents retrieves relevant events from episodic mem via recency (rule-based) + importance (reasoning-based) and relevance (emb/ dense). DocPrompting uses lib docs, i.e. dense retrieval frm semantic mem
- learning: write to long term memory
	- update episodic memory w experience (eg replay buffer in RL?)
	- Updating semantic memory with knowledge: use LLM to reason about raw experiences and store these into semantic memory, like gen agents reflection
	- Updating LLM parameters (procedural memory). eg via supervised, imitation learning. RL. RLHF, RLAIF. distillation
	- Updating agent code (procedural memory).
		- updating reasoning (eg prompt templates, prompt update)
		- updating grounding (eg voyager's curriculum library)
		- updating retrieval (query / document expansion, retrieval distillation to learn better retrieval over time)
		- updating learning or decision-making: authors not aware of related works tho this direction is possible, but note alignment issues
	- language agents can select frm diversity of learning procedures, allowing them to learn rapidly (potentially faster than fixed way of learning like updating model params) by storing task-relevant language, and leverage multiple forms of learning to compound self-improvement
	- authors emphasize that most work are on additive techniques for memory, and works that are subtractive or selective (eg modifying memories) are understudied
## Choose actions via decision making
retrieval + reasoning to plan by proposing + evaluating candidate learning or grounding actions. choose n execute the best action, observe, repeat cycle

- planning
	- proposal: generate action candidates via reasoning and retrieval. possibilities: all actions if action space is small, rule based selection, GOFAI planning eg PDDL
	- eval: via heuristic rules, LLM (perpelxity) values, learned values, LLM reasoning or some combination. "Particularly, LLM reasoning can help evaluate actions by internally simulating their grounding feedback from the external world". "identifying defects, and proposing modifications that address those defects" -- some code models do this, see CodeRL
	- selection: decide via values or decide to re-iterate to some proposal or evaluate sub-state
- execution
- recent trend to "investigate more complex decision making by proposing more than one action, and with iterative proposal and evaluation", such as tree of thoughts (iteratively propose and evaluate thoughts in a systematic tree search (DFS/BFS) to return a final solution), RAP (generates partial thoughts and maintains them in a tree structure using MCTS)

## Case studies
see table 2 for comparison across existing examples

selective table

| Paper    | LT memory | ext grounding | internal actions | decision making |
| -------- | ------- | --- | ---| --- |
| ReAct | - | digital | reason | propose|
| Voyager | procedural | digital |  reason/ retrieve/ learn | propose |
| Generative Agents |  episodic/semantic | digital/agent |  reason/ retrieve/ learn | propose |

---

# Other
- Draws parallels with these ideas: _production systems_ and _cognitive architectures_ (finally!)
	- "Production systems generate a set of outcomes by iteratively applying rules"
- Fig 1, the different types of LLMs and going up the hierarchy of complexity
    - 1a. Simplest type: LLM as generic input-output fn
    - 1b. LLM as an agent, in a typical RL fashion interacting with the env by producing an action based on observation
    - 1c. Cognitive LM, same setup as 1b but the agent internally has various components like memory, retrieval learning and reasoning (i.e. RAG style)
- Background: From Strings to Symbolic AGI
    - "Large production systems connected to external sensors, actuators, and knowledge bases required correspondingly sophisticated control flow."
    - "AI researchers defined 'cognitive architectures' that mimicked human cognition - explicitly instantiating processes such as perception, memory, planning (Adams et al., 2012) to achieve flexible, rational, real-time behaviors"
    - Canonical example: Soar (fig 2A). My summary [here](soar.md)
    - CAs have become less popular in the community due to the limitation to domains that can be prescribed by logical predicates, and require pre-specified rules to function
    - LLMs appear well-posed to meet these challenges due to its flexibility (can operate over arbitrary text) and reduced user specification requirements (LLMs learn a distribution over productions via pretraining)
    - Researchers have begun to use LLMs within CAs for their implicit world knowledge and to augment traditional symbolic approaches (see citation)
    - In this paper, the authors import principles from CA to guide the design of LLM based agents
- Connections b/w LMs and production systems
    - section draws analogies between the two. 
    - Fig 3 shows the evolution of LMs to agents. 3a: basic LLM call. 3b: prompt chaining uses pre defined sequences of LLM calls. 3c: language agents use an interactive feedback loop with env.
    
- Discussion
    - Planning vs. execution: how much should agents plan? "balancing the cost of planning against the utility of the resulting improved plan" (ref [xkcd comic](https://xkcd.com/1445/) of plan A vs plan B vs thinking about choosing plan A or B)
    - Learning vs. acting: how should agents continuously and autonomously learn? (explore-exploit for learning)
    - LLMs vs. code: where should agents rely on each? "CoALA thus suggests that good design uses agent code primarily to implement classic, generic planning algorithms - and relies heavily on the LLM for action proposal and evaluation."

---

# Soar overview
- Decision making: Fig 2B
	- each decision cycle: their preconditions are checked against the agent's working memory
	- proposal and evaluation phase: a set of productions is used to generate and rank a candidate set of possible actions (see operators vs rules footnote)
- Learning
	- Soar can write new productions into its procedural memory, effectively updating its source code!